{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PA4_Part1_21100121.ipynb","provenance":[{"file_id":"1Ae5reKJsYSQRX5bCckfFPs-RWphwpqeB","timestamp":1587904182207}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PYubDSiwGQAh"},"source":["# CS 436 CS5310 - Computer Vision - Assignment#4 - Part#1\n","\n","*__Submission Instructions:__*\n","- Rename this notebook to `PA4_rollnumber.ipynb` before submission on LMS.\n","- Code for all the tasks must be written in this notebook (you do not need to submit any other files).\n","- The output of all cells must be present in the version of the notebook you submit.\n","- The university honor code should be maintained. Any violation, if found, will result in disciplinary action."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UUlVZWsoGULv","outputId":"06f655a5-5487-44e0-dcfb-55b90799f983","executionInfo":{"status":"ok","timestamp":1588095504041,"user_tz":-300,"elapsed":3442,"user":{"displayName":"Taimoor Arif","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSDPOKp8T_gP0uIUEZdZ1vLrAH3CEqevo1Nt9n3Q=s64","userId":"03232148475366633594"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["#Executed on Colab\n","#Import all the required libraries\n","import keras\n","from keras.datasets import fashion_mnist\n","from keras.layers import Activation, Input, Embedding, LSTM, Dense, Lambda, GaussianNoise, concatenate\n","from keras.models import Model\n","import numpy as np\n","from keras.utils import np_utils\n","from keras.layers.core import Dense, Dropout, Activation\n","from keras import backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","from keras.optimizers import SGD, Adam, RMSprop\n","from keras.constraints import max_norm\n","from keras.layers import MaxPooling2D, Dropout, Dense, Flatten, Activation, Conv2D, GlobalAveragePooling2D\n","from keras.models import Sequential\n","from keras.losses import categorical_crossentropy as logloss\n","from keras.metrics import categorical_accuracy\n","from keras.applications import vgg16\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","from matplotlib import offsetbox\n","import cv2\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn')\n","import seaborn as sn\n","from sklearn.metrics import confusion_matrix"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4YNoEeQiGdGi"},"source":["## Overview\n","\n","In this assignment you will be exploring a few important concepts used in the deep learning projects:\n","- Training image classification algorithms using Deep Learning\n","- Dataset Analyses \n","- Testing deep learning classifier with the test data\n","- Fine-tuning / Transfer Learning\n","\n","We will be using the Fashion MNIST dataset that is provided by Keras. You will also be working with pretrained models, which can be downloaded from keras applications. You are **highly** encouraged to explore the images in dataset and model architectures in order to get the most out of this assignment. \n","\n","**_Dataset:_**\n","Fashion-MNIST is a dataset of  article images from a e-commerce website named zalando with 70,000 images of size (28x28x1) each belonging to one of the 10 classes.\n","\n","**_Pretrained Models:_** \n","Can be found [here](https://keras.io/applications/#applications)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1dSvdXobGlZy"},"source":["## Task 1: Data Preparation for transfer learning\n","\n","To prepare your data:\n","- Download the Fashion MNIST dataset\n","- Split it into train, validation, and test sets\n","- Convert the labels into categorical one hot encoded labels\n","- Cast the images as float and normalize the values between zero and one\n","- The Fashion MNIST dataset contains images of shape (28,28,1), the mimumm (height,width,channels) required by our pretrained model(in this case vgg-16) is (32,32,3)\n","- Convert the images in the datset to shape (32,32,3)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t3SNrR8VGaCP","outputId":"bc9724c8-15a7-4385-ecdf-1b0bc6135623","executionInfo":{"status":"ok","timestamp":1588095504648,"user_tz":-300,"elapsed":4028,"user":{"displayName":"Taimoor Arif","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSDPOKp8T_gP0uIUEZdZ1vLrAH3CEqevo1Nt9n3Q=s64","userId":"03232148475366633594"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["nb_classes = 10\n","#Download Fashion MNIST dataset(Done for you)\n","(X_Train, y_Train), (X_test, y_test) = fashion_mnist.load_data()\n","#Split the fashion MNIST dataset into train, validation and test sets (Done for you)\n","X_train, X_val, y_train, y_val = train_test_split(X_Train, y_Train, test_size=0.20)\n","\n","#Convert y_train,y_val and y_test to categorical binary values \n","#TO DO\n","y_train = keras.utils.to_categorical(y_train, nb_classes, dtype='float32')\n","y_val = keras.utils.to_categorical(y_val, nb_classes, dtype='float32')\n","y_test = keras.utils.to_categorical(y_test, nb_classes, dtype='float32')\n","#Hint\n","#See function \"np_utils.to_categorical()\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_Lc7sFV0HAK5","colab":{}},"source":["#Reshape your dataset to batch_size, width, height, #channels(Done for you)\n","X_train = X_train.reshape(48000, 28, 28, 1)\n","X_val = X_val.reshape(12000, 28, 28, 1)\n","X_test = X_test.reshape(10000, 28, 28, 1)\n","\n","#Convert your dataset to float type\n","#TO DO\n","X_train = X_train.astype('float32')\n","X_val = X_val.astype('float32')\n","X_test = X_test.astype('float32')\n","#Normalize the values\n","X_train = cv2.normalize(X_train, None, 0, 255, cv2.NORM_MINMAX)\n","X_val = cv2.normalize(X_val, None, 0, 255, cv2.NORM_MINMAX)\n","X_test = cv2.normalize(X_test, None, 0, 255, cv2.NORM_MINMAX)\n","\n","#TO DO"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_3SnJ0dRHO_f","colab":{}},"source":["#Preprocess the data for our pretrained model(requires 3 channels) and a mimimum height and width of 32.\n","#Convert all images to shape (32,32,3)\n","X_train_new=[]\n","X_val_new=[]\n","\n","X_train_new\n","\n","size = len(X_train)\n","\n","for i in range(size):\n","  X_train_new.append(cv2.resize(X_train[i], (32, 32)))\n","  val = np.empty((32, 32, 3), dtype = 'float32')\n","  for j in range(32):\n","    for k in range(32):\n","      val[j][k][0] = X_train_new[i][j][k] \n","      val[j][k][1] = X_train_new[i][j][k]\n","      val[j][k][2] = X_train_new[i][j][k]\n","  X_train_new[i] = val\n","\n","size = len(X_val)\n","for i in range(size):\n","  X_val_new.append(cv2.resize(X_val[i], (32, 32)))\n","  val = np.empty((32, 32, 3), dtype = 'float32')\n","  for j in range(32):\n","    for k in range(32):\n","      val[j][k][0] = X_val_new[i][j][k] \n","      val[j][k][1] = X_val_new[i][j][k]\n","      val[j][k][2] = X_val_new[i][j][k]\n","  X_val_new[i] = val\n","\n","\n","#TO DO"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"daOrE-w3Hd_P","colab":{}},"source":["def add_new_last_layer(base_model, nb_classes):\n","    \"\"\"Add last layer to the convnet\n","    Args:\n","    base_model: keras model excluding top layer\n","    nb_classes: number of classes\n","    Returns:\n","    new keras model with last fully connected layer\n","    \"\"\"\n","    FC_SIZE=1024\n","    x = base_model.output\n","    x = GlobalAveragePooling2D()(x)\n","    x = Dense(FC_SIZE, activation='relu')(x) \n","    predictions = Dense(nb_classes, activation='softmax')(x) \n","    model = Model(input=base_model.input, output=predictions)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XAg5AHSrHuPa"},"source":["## Task 2: Transfer Learning\n","\n","Next you will employ Transfer Learning and finetune the pretrained vgg-16 model to better fit the Fashion MNIST data for 10 classes. You will:\n","\n","- Change the number of nodes in the last FC layer according to the number of classes i.e. 10 \n","- Freeze everything except the FC layers and train it using the train split of our dataset (using appropriate hyperparameters), validating the network for validation split of data.\n","- Plot loss/accuracy vs epochs curves for your simulation\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sQJIzOqCHvUG","outputId":"63a7ce60-6552-4e6f-a0bf-b1e6942d6348","executionInfo":{"status":"ok","timestamp":1588095666369,"user_tz":-300,"elapsed":165716,"user":{"displayName":"Taimoor Arif","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSDPOKp8T_gP0uIUEZdZ1vLrAH3CEqevo1Nt9n3Q=s64","userId":"03232148475366633594"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Load the keras base model excluding the top layer\n","#TO DO\n","#Hint\n","#base_model=vgg16.VGG16(...)\n","\n","base_model = vgg16.VGG16(include_top = False, weights = 'imagenet', input_tensor = None, input_shape = None, pooling = None, classes = nb_classes)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 2s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZB0WCgrSH-PU","outputId":"b4c07fd1-b246-4367-b60c-87e8f96766b0","executionInfo":{"status":"ok","timestamp":1588095666371,"user_tz":-300,"elapsed":165707,"user":{"displayName":"Taimoor Arif","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSDPOKp8T_gP0uIUEZdZ1vLrAH3CEqevo1Nt9n3Q=s64","userId":"03232148475366633594"}},"colab":{"base_uri":"https://localhost:8080/","height":935}},"source":["#Since we will be using the pretrained weights for the image net dataset we will freeze all layers of our base model(Done for you)\n","for layer in base_model.layers:\n","    layer.trainable = False\n","#We will now add a new last layer according to the number of classes in our dataset using the \"add_new_last_layer()\" function provided to you\n","#TO DO\n","#Hint\n","#vgg_imagenet_new=add_new_last_layer(base_model,...)\n","vgg_imagenet_new = add_new_last_layer(base_model, nb_classes)\n","\n","#Print Model Summary(Done for you)\n","vgg_imagenet_new.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, None, None, 3)     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n","_________________________________________________________________\n","global_average_pooling2d_1 ( (None, 512)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1024)              525312    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                10250     \n","=================================================================\n","Total params: 15,250,250\n","Trainable params: 535,562\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8N0pekdtJndS","colab":{}},"source":["#Intialize Optimizer for our model(Done for you). You can try using different optimizers with different values\n","adam=Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","#Compile Model\n","#TO DO\n","#Hint\n","#vgg_imagenet_new.compile(optimizer=adam,...)\n","vgg_imagenet_new.compile(optimizer = adam, loss = logloss, metrics = ['accuracy'], weighted_metrics = None, target_tensors = None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BkIIzVeeKJbf","outputId":"7f6e5e9c-da40-41c6-e1a8-5780ccd5c17b","executionInfo":{"status":"ok","timestamp":1588095785329,"user_tz":-300,"elapsed":284643,"user":{"displayName":"Taimoor Arif","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSDPOKp8T_gP0uIUEZdZ1vLrAH3CEqevo1Nt9n3Q=s64","userId":"03232148475366633594"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#Train the vgg-16 model\n","epochs = 5\n","batch_size = 32 \n","#TO DO\n","#Hint\n","#history=vgg_imagenet_new.fit(...)\n","history = vgg_imagenet_new.fit(x = np.array(X_train_new), y = y_train, batch_size = batch_size, epochs = epochs,  validation_data = (np.array(X_val_new), y_val))\n","#The history variable will be used later to plot accuracy and loss curves"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Train on 48000 samples, validate on 12000 samples\n","Epoch 1/5\n","48000/48000 [==============================] - 29s 602us/step - loss: 0.9227 - accuracy: 0.7976 - val_loss: 0.4757 - val_accuracy: 0.8307\n","Epoch 2/5\n","48000/48000 [==============================] - 22s 464us/step - loss: 0.4330 - accuracy: 0.8461 - val_loss: 0.4724 - val_accuracy: 0.8362\n","Epoch 3/5\n","48000/48000 [==============================] - 22s 466us/step - loss: 0.3970 - accuracy: 0.8588 - val_loss: 0.4846 - val_accuracy: 0.8361\n","Epoch 4/5\n","48000/48000 [==============================] - 22s 465us/step - loss: 0.3771 - accuracy: 0.8644 - val_loss: 0.4852 - val_accuracy: 0.8425\n","Epoch 5/5\n","48000/48000 [==============================] - 22s 464us/step - loss: 0.3495 - accuracy: 0.8745 - val_loss: 0.4893 - val_accuracy: 0.8403\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CtG-N7iDKjAF","colab":{}},"source":["#Lets test our transfer learned model on the test set\n","#Preprocess the test data just like we did before i.e convert all images to shape (32,32,3)\n","X_test_new = []\n","size = len(X_test)\n","for i in range(size):\n","  X_test_new.append(cv2.resize(X_test[i], (32, 32)))\n","  val = np.empty((32, 32, 3), dtype ='float32')\n","  for j in range(32):\n","    for k in range(32):\n","      val[j][k][0] = X_test_new[i][j][k] \n","      val[j][k][1] = X_test_new[i][j][k]\n","      val[j][k][2] = X_test_new[i][j][k]\n","  X_test_new[i] = val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_qvuVJaALC2J","colab":{}},"source":["#Evaluate the transfer learned model\n","#TO DO\n","#Hint\n","#vgg_imagenet_new.evaluate(...)\n","vgg_imagenet_new.evaluate(x = np.array(X_test_new), y = y_test, batch_size = 32, sample_weight=None, steps=None, callbacks=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"I9jenvyLLReE","colab":{}},"source":["#Plot training and validation accuracy curves using matplot lib\n","#Hint\n","#Values for training and validation accuarcy can be obtained using history.history['accuracy'] and history.history['val_accuracy'] respectively\n","training_accuracy = history.history['accuracy']\n","validation_accuracy = history.history['val_accuracy']\n","\n","plt.plot(training_accuracy)\n","plt.plot(validation_accuracy)\n","plt.show"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hB3OLXYgL41Y","colab":{}},"source":["#Plot training and validation loss curves using matplot lib\n","#Hint\n","#Values for training and validation loss can be obtained using history.history['loss'] and history.history['val_loss'] respectively\n","training_loss = history.history['loss']\n","validation_loss = history.history['val_loss']\n","\n","plt.plot(training_loss)\n","plt.plot(validation_loss)\n","plt.show"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4XATeh2MMHMe"},"source":["## Task 3: Network Evaluation\n","\n","Next you will test your finetuned model by plotting a confusion matrix between classes predicted. You will:\n","\n","- Test your model for images in the test set\n","- Construct a multiclass confusion matrix (for 10 classes) for actual and predicted class of each image and visualize the confusion matrix as a heatmap\n","\n","*You can use scikit-learn's `metrics.confusion_matrix` function. Consult the relevant documentation.* "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F5oBJSb9MGIn","colab":{}},"source":["y_true = y_test.argmax(axis = 1)\n","y_pred = vgg_imagenet_new.predict(np.array(X_test_new), verbose = 1).argmax(axis = 1)\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","\n","sn.heatmap(conf_matrix, xticklabels = [0, 1, 2, 3, 4, 5 ,6 ,7, 8, 9], yticklabels = [0, 1, 2, 3, 4, 5 ,6 ,7, 8, 9])\n","plt.show"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtwTVsDqoP80","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}